{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IAMDSVSSANGRAL/Capstone--Project/blob/main/appliance_energy_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 -Samadhan**\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective:\n",
        "The objective of this project is to develop a regression model that accurately predicts the energy consumption of household appliances based on various input features. The model aims to provide insights into energy usage patterns and facilitate energy efficiency improvements in residential settings.\n",
        "\n",
        "Data:\n",
        "The project utilizes a dataset that contains information on household appliance energy consumption along with several relevant input features. The dataset includes variables such as temperature, humidity, time of day, and various appliance power readings. The data is collected over a specific time period and is representative of real-world residential energy usage scenarios.\n",
        "\n",
        "Tasks:\n",
        "\n",
        "Exploratory Data Analysis (EDA):\n",
        "\n",
        "Perform a thorough analysis of the dataset to understand the distribution, statistics, and relationships among variables.\n",
        "Identify any missing values, outliers, or data quality issues that need to be addressed.\n",
        "Visualize the data using appropriate charts and graphs to gain insights into the patterns and trends.\n",
        "Data Preprocessing:\n",
        "\n",
        "Handle missing values by applying suitable imputation techniques or deciding on appropriate strategies for dealing with them.\n",
        "Address outliers and anomalies by considering various methods such as removal, transformation, or capping.\n",
        "Normalize or scale the data if necessary to ensure all features are on a similar scale.\n",
        "Feature Engineering:\n",
        "\n",
        "Explore the relationships between the input features and the target variable (appliance energy consumption) to identify potential feature engineering opportunities.\n",
        "Create new features, derive meaningful variables, or transform existing variables to capture important patterns or interactions in the data.\n",
        "Model Development:\n",
        "\n",
        "Split the dataset into training and testing sets for model development and evaluation.\n",
        "Select an appropriate regression algorithm (e.g., linear regression, decision tree regression, random forest regression) based on the project requirements and characteristics of the data.\n",
        "Train the model using the training data and tune hyperparameters to optimize performance.\n",
        "Evaluate the model's performance using various metrics such as mean squared error (MSE), mean absolute error (MAE), and R-squared.\n",
        "Model Evaluation and Interpretation:\n",
        "\n",
        "Assess the model's performance on the testing data to measure its ability to generalize to unseen data.\n",
        "Interpret the model's coefficients or feature importance to gain insights into the factors that have the most significant impact on appliance energy consumption.\n",
        "Validate the model's predictions against domain knowledge or external benchmarks to ensure its reliability and usefulness.\n",
        "Model Deployment and Recommendations:\n",
        "\n",
        "Deploy the trained model into a production environment or create a user-friendly interface for stakeholders to interact with the model.\n",
        "Provide recommendations based on the model's predictions and insights to improve energy efficiency, optimize appliance usage, or suggest modifications in residential settings.\n",
        "Conclusion:\n",
        "The Appliance Energy Prediction regression project aims to develop a robust regression model to accurately predict household appliance energy consumption. By analyzing and understanding the data, performing feature engineering, and building an effective regression model, the project provides valuable insights and recommendations for optimizing energy usage and promoting energy-efficient practices in residential settings.\n",
        "\n",
        "Note: This project summary provides a general outline and can be tailored based on specific requirements, dataset characteristics, and project goals."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/IAMDSVSSANGRAL/applianceenergyprediction"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -"
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "\n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "\n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "\n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "\n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from pandas.plotting import scatter_matrix\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "from datetime import datetime as dt\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the data set\n",
        "data_raw = pd.read_csv('/content/drive/MyDrive/Santa/Regression capstone/data_application_energy.csv')"
      ],
      "metadata": {
        "id": "TmkdQzZojsHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a copy of data set\n",
        "data = data_raw.copy()"
      ],
      "metadata": {
        "id": "HJ8f2WTWL-YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "num_rows, num_cols = data.shape\n",
        "\n",
        "print(\"Number of rows:\", num_rows)\n",
        "print(\"Number of columns:\", num_cols)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your date column is named \"date_column\"\n",
        "data['date'] = pd.to_datetime(data['date'])\n"
      ],
      "metadata": {
        "id": "76kdxS8WdrbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting date as the index:\n",
        "data.set_index('date', inplace=True)"
      ],
      "metadata": {
        "id": "5kPxyGdERI8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count assinged a dataframe name 'df'\n",
        "df = data[data.duplicated()]"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#There is no duplicate rows in the data\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bVN4H8UU09Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "data.isna().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import missingno as msno\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the null matrix\n",
        "msno.matrix(data)\n",
        "\n",
        "# Customizing the plot\n",
        "plt.title('Null Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We examine the data and found out that there is no Duplicate rows in the data set.\n",
        "\n",
        "We also came to know that there is no null values in the data set.\n",
        "\n",
        "While examining the data type, Date feature got data type as an object which is expected to be datetime ."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "data.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The observation data consists of the following variables:**\n",
        "\n",
        "\n",
        "datetime year-month-day hour : minute:second\n",
        "\n",
        "Appliances: energy use in Wh [TARGETED]\n",
        "\n",
        "lights: energy use of light fixtures in the house in Wh\n",
        "\n",
        "T1: Temperature in kitchen area, in Celsius\n",
        "\n",
        "RH_1: Humidity in kitchen area, in %\n",
        "\n",
        "T2: Temperature in living room area, in Celsius\n",
        "\n",
        "RH_2:Humidity in living room area, in %\n",
        "\n",
        "T3:Temperature in laundry room area\n",
        "\n",
        "RH_3:Humidity in laundry room area, in %\n",
        "\n",
        "T4:Temperature in office room, in Celsius\n",
        "\n",
        "RH_4:Humidity in office room, in %\n",
        "\n",
        "T5:Temperature in bathroom, in Celsius\n",
        "\n",
        "RH_5:Humidity in bathroom, in %\n",
        "\n",
        "T6:Temperature outside the building (north side), in Celsius\n",
        "\n",
        "RH_6:Humidity outside the building (north side), in %\n",
        "\n",
        "T7:Temperature in ironing room , in Celsius\n",
        "\n",
        "RH_7:Humidity in ironing room, in %\n",
        "\n",
        "T8:Temperature in teenager room 2, in Celsius\n",
        "\n",
        "RH_8:Humidity in teenager room 2, in %\n",
        "\n",
        "T9:Temperature in parents room, in Celsius\n",
        "\n",
        "RH_9:Humidity in parents room, in %\n",
        "\n",
        "T_out:Temperature outside (from Chièvres weather station), in Celsius\n",
        "\n",
        "Press_mm_hg: (from Chièvres weather station), in mm Hg\n",
        "\n",
        "RH_out: Humidity outside (from Chièvres weather station), in %\n",
        "\n",
        "Windspeed: (from Chièvres weather station), in m/s\n",
        "\n",
        "Visibility: (from Chièvres weather station), in km\n",
        "\n",
        "Tdewpoint: (from Chièvres weather station), °C\n",
        "\n",
        "rv1: Random variable 1, nondimensional\n",
        "\n",
        "rv2: Rnadom variable 2, nondimensional"
      ],
      "metadata": {
        "id": "PYzlcej3OU5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Unique Values count for each variable.\n",
        "for i in data.columns.tolist():\n",
        "  print(\"The unique values in\",i, \"is\",data[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "LGAz2eAYwhmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Round the unique values to two decimal places\n",
        "rounded_unique_values = data.apply(lambda x: set(round(val, 2) for val in x))\n",
        "\n",
        "# Print the unique values for each feature\n",
        "for feature, unique in rounded_unique_values.items():\n",
        "    print(f'{feature}: {unique}')"
      ],
      "metadata": {
        "id": "_6QYnjVk4wNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separating columns:\n",
        "temperature_column = [i for i in data.columns if \"T\" in i]\n",
        "humidity_column = [i for i in data.columns if \"RH\" in i]\n",
        "other = [i for i in data.columns if (\"T\" not in i)&(\"RH\" not in i)]"
      ],
      "metadata": {
        "id": "Kst3KQHVRrVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[temperature_column].describe(include='all')"
      ],
      "metadata": {
        "id": "DzXNlgYLRtMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[humidity_column].describe()"
      ],
      "metadata": {
        "id": "w_j4XCxISHY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[other].describe()"
      ],
      "metadata": {
        "id": "StZKZkcwSQpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first, second and third quartiles are 0 for the lights column, which means that most of the information of this column is 0.\n",
        "\n",
        "**prescence of outlier in other columns**\n",
        "\n",
        "Looking at the statistics of \"other\" columns, we can see that there are some outliers in Visibility, Windspeed and Appliance."
      ],
      "metadata": {
        "id": "fg_iUznxVuXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting values of the \"lights\" column:\n",
        "data['lights'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "4WZ0JUpVUwPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "77% value of lights column are 0 and it is not relevant for prediction. so we are going to drop this column"
      ],
      "metadata": {
        "id": "TBGBIpteVhXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the lights column:\n",
        "data.drop(columns='lights', inplace=True)"
      ],
      "metadata": {
        "id": "VJbGWhxOVcbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the outlier in dataset\n",
        "# Assuming 'data' is your DataFrame\n",
        "num_columns = len(data.columns)\n",
        "fig, axes = plt.subplots(nrows=num_columns, figsize=(8, num_columns*4))\n",
        "\n",
        "for i, column in enumerate(data.columns):\n",
        "    data.boxplot(column=column, ax=axes[i])\n",
        "    axes[i].set_title(f'Box Plot for {column}')\n",
        "    axes[i].set_xlabel('Column')\n",
        "    axes[i].set_ylabel('Values')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mtDVU5Wu54Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#close look pon three columns\n",
        "fig_sub = make_subplots(rows=1, cols=3, shared_yaxes=False)\n",
        "\n",
        "fig_sub.add_trace(go.Box(y=data['Appliances'].values,name='Appliances'),row=1, col=1)\n",
        "fig_sub.add_trace(go.Box(y=data['Windspeed'].values,name='Windspeed'),row=1, col=2)\n",
        "fig_sub.add_trace(go.Box(y=data['Visibility'].values,name='Visibility'),row=1, col=3)\n",
        "fig_sub.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GPeXc9FMZV3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#AUTOEDA\n",
        "!pip install sweetviz\n",
        "import sweetviz as sv\n",
        "sweet_report = sv.analyze(data)\n",
        "sweet_report.show_html('sweet_report.html')"
      ],
      "metadata": {
        "id": "4hKGGwU4HsqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new features\n",
        "data['month'] = data.index.month\n",
        "data['weekday'] = data.index.weekday\n",
        "data['hour'] = data.index.hour\n",
        "data['week'] = data.index.week\n",
        "data['day'] = data.index.day\n",
        "data['day_of_week'] = data.index.dayofweek"
      ],
      "metadata": {
        "id": "uV5xXflu9FU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pivot table to aggregate the daily energy consumption\n",
        "daily_energy = data.pivot_table(values='Appliances', index='day', columns='month', aggfunc = np.mean)\n",
        "\n",
        "# Create a heatmap using the pivot table\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title('Daily Energy Consumption')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Day')\n",
        "plt.imshow(daily_energy, cmap='YlGnBu', aspect='auto')\n",
        "plt.colorbar(label='Energy Consumption')\n",
        "plt.xticks(range(0,5), ['Jan', 'Feb', 'Mar', 'Apr', 'May'])\n",
        "plt.yticks(range(1, 32))\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xSZHw-DN80Fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I choose this chart to identify the distribution of each variable in the data.\n"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map the day of the week values to their respective names\n",
        "day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "data['day_of_week'] = data['day_of_week'].map(lambda x: day_names[x])\n",
        "\n",
        "# Create a box plot or violin plot to compare energy consumption across different days of the week\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='day_of_week', y='Appliances', data=data, order=day_names)  # or sns.violinplot()\n",
        "plt.title('Appliance Energy Consumption by Day of the Week')\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Energy Consumption')"
      ],
      "metadata": {
        "id": "vFnJADwHDwAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a line plot to show the trend of energy consumption over time\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(data.index, data['Appliances'])\n",
        "plt.title('Energy Consumption of Appliances Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Energy Consumption')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing distributions using Histograms:\n",
        "data.hist(figsize=(17, 20), grid=False);"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "correlation_matrix = data.corr()\n",
        "plt.figure(figsize=(18, 15))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"RdYlGn\")\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OtkZZyU1fiOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reorder the data for clear vision\n",
        "desired_order = [\"T1\",\"T2\",\"T3\",\"T4\",\"T5\",\"T6\",\"T7\",\"T8\",\"T9\",\"T_out\",\"Tdewpoint\",\"RH_1\",\"RH_2\",\"RH_3\",\"RH_4\",\"RH_7\",\"RH_8\",\"RH_9\",\"RH_6\",\"RH_5\",\"RH_out\",\"Press_mm_hg\",\n",
        "                \"Windspeed\",\"Visibility\",\"rv1\", \"rv2\",\"Appliances\"]\n",
        "#assinging new_data as new name of dataframe\n",
        "new_data = data.reindex(columns=desired_order)"
      ],
      "metadata": {
        "id": "er1SqrSK2g8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "correlation_matrix = new_data.corr()\n",
        "plt.figure(figsize=(18, 15))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"RdYlGn\")\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the list of column names in your dataset\n",
        "columns = data.columns\n",
        "\n",
        "# Determine the number of rows and columns for subplots\n",
        "num_rows = len(columns)\n",
        "num_cols = 1\n",
        "\n",
        "# Create subplots with specified number of rows and columns\n",
        "fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(10, 80))\n",
        "\n",
        "# Iterate over each column (excluding \"Appliances\") and create pair plot\n",
        "for i, column in enumerate(columns):\n",
        "    #if column != \"Appliances\":\n",
        "        sns.scatterplot(data=data, x=\"Appliances\", y=column, ax=axes[i])\n",
        "        axes[i].set_xlabel(\"Appliances\")\n",
        "        axes[i].set_ylabel(column)\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pzVpJ9VlZ_qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-MMp3DFQLEqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is huge prescece of heteroscedasticity and we usually do log tranformation to solve this error."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no significant linear relationship between the independent variables and the appliance energy consumption.\n",
        "\n",
        "Alternative Hypothesis (H1): There is a significant linear relationship between the independent variables and the appliance energy consumption."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "OBBrJOQ5cjCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Extract the two continuous variables you want to test\n",
        "column_to_drop = ['Appliances','day_of_week']\n",
        "independent_variables = data.drop(column_to_drop, axis = 1)\n",
        "dependent_variable = data['Appliances']\n",
        "\n",
        "# Step 2: Perform the Correlation Test (Pearson correlation)\n",
        "correlation_coefficients, p_values = [], []\n",
        "for feature in independent_variables.columns:\n",
        "    correlation_coefficient, p_value = pearsonr(independent_variables[feature], dependent_variable)\n",
        "    correlation_coefficients.append(correlation_coefficient)\n",
        "    p_values.append(p_value)\n",
        "\n",
        "# Step 3: Interpret the Results for each feature\n",
        "alpha = 0.05  # Significance level (commonly set to 0.05)\n",
        "for i, feature in enumerate(independent_variables.columns):\n",
        "    print(f\"Correlation Coefficient for '{feature}': {correlation_coefficients[i]:.4f}\")\n",
        "    print(f\"P-value for '{feature}': {p_values[i]:.4f}\")\n",
        "\n",
        "    if p_values[i] < alpha:\n",
        "        print(\"Result: The correlation is statistically significant (reject H0).\\n\")\n",
        "    else:\n",
        "        print(\"Result: There is no significant correlation (fail to reject H0).\\n\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the practical implementation provided earlier, the statistical test used to obtain the p-value is the Pearson correlation coefficient test. The Pearson correlation coefficient, also known as Pearson's r or simply r, is a measure of the linear relationship between two continuous variables."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The p-value obtained from the test indicates the probability of observing the calculated correlation coefficient (or a more extreme value) if the null hypothesis is true. The null hypothesis (H0) in this context states that there is no significant linear relationship between the two variables.\n",
        "\n",
        "By comparing the p-value to a chosen significance level (alpha), commonly set to 0.05 (5%), we can determine whether to reject or fail to reject the null hypothesis. If the p-value is less than alpha, we reject the null hypothesis, suggesting a statistically significant correlation. If the p-value is greater than alpha, we fail to reject the null hypothesis, indicating no significant correlation.\n",
        "\n",
        "This test is appropriate when you want to assess the strength and direction of the linear relationship between two continuous variables. It is commonly used to explore the association between variables in correlation analysis and is widely used in various fields of research and data analysis."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thankfully there is no missing value in out dataset"
      ],
      "metadata": {
        "id": "qOnQ9TPjpJTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have not used any missing values handling technique as there are no Nan Values in the data set"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "WZAWrTlQJ688"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = ['day_of_week','rv1','rv2','hour']\n",
        "data.drop(columns_to_drop, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "C_kdVVTSKQRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####variance threshold"
      ],
      "metadata": {
        "id": "fvhVroMTIxHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''from sklearn.feature_selection import VarianceThreshold\n",
        "var = VarianceThreshold(threshold=0.0)\n",
        "var.fit(data)'''"
      ],
      "metadata": {
        "id": "mSIDocJMIweU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''var.get_support()'''"
      ],
      "metadata": {
        "id": "mLc2R2eYKkZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''data.columns[var.get_support()]'''"
      ],
      "metadata": {
        "id": "BU7yt76YMo7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''constant_column = [column for column in data.columns if column not in data.columns[var.get_support()]]'''"
      ],
      "metadata": {
        "id": "RHS3geyTLS1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''print(len(constant_column))'''"
      ],
      "metadata": {
        "id": "vzapWDuIL_1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''for feature in constant_column:\n",
        "  print(feature)'''"
      ],
      "metadata": {
        "id": "EgSneo7XMI8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###finding the skewed and symmetrical data"
      ],
      "metadata": {
        "id": "8N_Na8hJcJcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#examining the skewness in the dataset to check the distribution\n",
        "skewness = data.skew()\n",
        "print(skewness)\n",
        "\n",
        "#ginding the absolute value\n",
        "abs(skewness)\n",
        "\n",
        "# setting up the threshold\n",
        "skewness_threshold = 0.5\n",
        "\n",
        "# Separate features into symmetrical and skewed based on skewness threshold\n",
        "symmetrical_features = skewness[abs(skewness) < skewness_threshold].index\n",
        "skewed_features = skewness[abs(skewness) >= skewness_threshold].index\n",
        "\n",
        "#printing the features\n",
        "print(symmetrical_features)\n",
        "print(skewed_features)\n",
        "\n",
        "# Create new DataFrames for symmetrical and skewed features\n",
        "symmetrical_data = data[symmetrical_features]\n",
        "skewed_data = data[skewed_features]\n"
      ],
      "metadata": {
        "id": "1fSKxWkHCi-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skewed_data.drop('Appliances',axis = 1,inplace = True)"
      ],
      "metadata": {
        "id": "dIlgOh5tGoRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skewed_data"
      ],
      "metadata": {
        "id": "9f_lt6SWHEGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the liabrary\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# Initialize the PowerTransformer\n",
        "power_transformer = PowerTransformer()\n",
        "\n",
        "# Fit and transform the data using the PowerTransformer\n",
        "power_transformed = pd.DataFrame(power_transformer.fit_transform(skewed_data))\n",
        "power_transformed.columns = skewed_data.columns\n"
      ],
      "metadata": {
        "id": "XYFTzJnZHRA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "power_transformed"
      ],
      "metadata": {
        "id": "_lWwTodiIVeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the index to the default integer index\n",
        "symmetrical_data.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "wIsZBYKLKSxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "symmetrical_data"
      ],
      "metadata": {
        "id": "FqbmBhCgOSwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate horizontally (along columns)\n",
        "tranformed_data = pd.concat([symmetrical_data, power_transformed], axis=1)"
      ],
      "metadata": {
        "id": "-axrMYS-Ju78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tranformed_data"
      ],
      "metadata": {
        "id": "XYxVg0eOJ7De"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Yes My data needs transformation specially skewed data , i used power transformaiton to solve this concern"
      ],
      "metadata": {
        "id": "dMFDTrcIcfEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Scaling the DATA set"
      ],
      "metadata": {
        "id": "yxxTpRupBq2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the desired liabrary\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = pd.DataFrame(scaler.fit_transform(tranformed_data))\n",
        "scaled_data.columns = tranformed_data.columns\n",
        "scaled_data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''# DImensionality Reduction (If needed)\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize the data\n",
        "#scaler = StandardScaler()\n",
        "#scaled_data = scaler.fit_transform(new_data)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2)  # Specify the number of co\n",
        "\n",
        "principal_components = pca.fit_transform(scaled_data)\n",
        "\n",
        "# Access the principal components and explained variance ratio\n",
        "components = pca.components_\n",
        "explained_variance_ratio = pca.explained_variance_ratio_'''\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = scaled_data\n",
        "y = data['Appliances']"
      ],
      "metadata": {
        "id": "bNJtj0r0CDnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=3)"
      ],
      "metadata": {
        "id": "as7VuE-HPHsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "70/30 Split: This ratio involves splitting the data into 70% for training and 30% for testing. It is a commonly used ratio when there is a sufficient amount of data available. The larger portion is used for training the model, while the smaller portion is used for evaluating its performance."
      ],
      "metadata": {
        "id": "noUyOxn2559T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 - Simple Linear Regression Model"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the mdoel\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "#defining the object\n",
        "reg = LinearRegression()\n",
        "reg.fit(x_train, y_train)\n",
        "\n",
        "#training dataset score\n",
        "training_score = reg.score(x_train, y_train)\n",
        "\n",
        "#predicting the value\n",
        "y_pred = reg.predict(x_test)\n",
        "\n",
        "#visual of training score\n",
        "print(\"Train score:\" ,training_score)\n",
        "\n",
        "#calculating the testing accuracy\n",
        "MSE  = mean_squared_error((y_test),(y_pred))\n",
        "print(\"Test MSE :\" , MSE)\n",
        "\n",
        "r2 = r2_score((y_test),(y_pred))\n",
        "print(\"Test R2 :\" ,r2)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "sns.displot(y_pred - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.array(y_test))\n",
        "plt.plot(y_pred)\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3UPe56CAxrZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "\n",
        "# Create a Linear Regression model (you can replace this with any other regression model)\n",
        "model = LinearRegression()\n",
        "\n",
        "# Define hyperparameter search space (you can customize this based on your model)\n",
        "param_dist = {'fit_intercept': [True, False],\n",
        "              'copy_X': [True, False],\n",
        "              'positive':[True, False]}\n",
        "\n",
        "# Perform RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist,\n",
        "                                   n_iter=10, scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the RandomizedSearchCV to find the best hyperparameters\n",
        "random_search.fit(x_train, y_train)\n",
        "\n",
        "# Get the best hyperparameters and model\n",
        "best_params = random_search.best_params_\n",
        "best_model = random_search.best_estimator_\n",
        "\n",
        "# Train the best model with the entire training dataset\n",
        "best_model.fit(x_train, y_train)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "test_predictions = best_model.predict(x_test)\n",
        "\n",
        "# Calculate evaluation metrics for the test predictions (e.g., mean squared error)\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(y_test, test_predictions)\n",
        "r2 = r2_score((y_test),(test_predictions))\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Test MSE:\", mse)\n",
        "print(\"Test R2:\", r2)\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.score(x_train, y_train)"
      ],
      "metadata": {
        "id": "vH1fqu4hgP-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(test_predictions - y_test,kind ='kde')"
      ],
      "metadata": {
        "id": "SkDXF8syg0iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - Polynomial Regression model\n"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "ymH6IB2GjXg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "X = data.drop(columns=['Appliances'],axis= 1)\n",
        "y = data['Appliances']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Specify the degree of polynomial (you can change this based on your data)\n",
        "degree = 2\n",
        "\n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=degree)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "X_test_poly = poly_features.transform(X_test)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model using the polynomial features\n",
        "model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = model.predict(X_train_poly)\n",
        "test_predictions = model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Polynomial Regression (Degree {}):\".format(degree))\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)\n",
        "\n",
        "# Plot the actual vs. predicted values for the test set\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted values (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "S2z9RrxpzvVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let just try to implement the same model with degree = 3 , to ehance the things"
      ],
      "metadata": {
        "id": "fGBlQcqOtkRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "X = data.drop(columns=['Appliances'],axis= 1)\n",
        "y = data['Appliances']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Specify the degree of polynomial (you can change this based on your data)\n",
        "degree = 3\n",
        "\n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=degree)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "X_test_poly = poly_features.transform(X_test)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "model = LinearRegression()\n",
        "\n",
        "# Train the model using the polynomial features\n",
        "model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = model.predict(X_train_poly)\n",
        "test_predictions = model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Polynomial Regression (Degree {}):\".format(degree))\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)\n",
        "\n",
        "# Plot the actual vs. predicted values for the test set\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted values (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TiWa-P60qOiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 - RIDGE Regression Model"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import  Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "X = data.drop(columns=['Appliances'],axis= 1)\n",
        "y = data['Appliances']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Specify the degree of polynomial (you can change this based on your data)\n",
        "degree = 2\n",
        "\n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=degree)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "X_test_poly = poly_features.transform(X_test)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "ridge_model = Ridge(alpha=1.0)\n",
        "\n",
        "# Train the model using the polynomial features\n",
        "ridge_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = ridge_model.predict(X_train_poly)\n",
        "test_predictions = ridge_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Polynomial Regression (Degree {}):\".format(degree))\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)\n",
        "\n",
        "# Plot the actual vs. predicted values for the test set\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted values (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eF6mZ2-oveGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n"
      ],
      "metadata": {
        "id": "aSkQDwTWxvud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create a Ridge Regression model\n",
        "ridge_model = Ridge()\n",
        "\n",
        "# Perform Cross-Validation and Hyperparameter Tuning\n",
        "param_grid = {'alpha': [0.1, 1.0, 10.0]}  # Define the hyperparameter grid\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=ridge_model, param_grid=param_grid,\n",
        "                           scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV to find the best degree and alpha\n",
        "grid_search.fit(X_train_poly, y_train)\n",
        "\n",
        "# Get the best degree and alpha from the GridSearchCV results\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = best_model.predict(X_train_poly)\n",
        "test_predictions = best_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Best Alpha:\", best_alpha)\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)\n",
        "\n",
        "# Plot the actual vs. predicted values for the test set\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted values (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4 - Lasso Regression Model"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import  Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "X = data.drop(columns=['Appliances'],axis= 1)\n",
        "y = data['Appliances']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Specify the degree of polynomial (you can change this based on your data)\n",
        "degree = 2\n",
        "\n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=degree)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "X_test_poly = poly_features.transform(X_test)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "lasso_model = Lasso(alpha=1.0)\n",
        "\n",
        "# Train the model using the polynomial features\n",
        "lasso_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = lasso_model.predict(X_train_poly)\n",
        "test_predictions = lasso_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Polynomial Regression (Degree {}):\".format(degree))\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)\n",
        "\n",
        "# Plot the actual vs. predicted values for the test set\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted values (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create a Ridge Regression model\n",
        "lasso_model = Lasso()\n",
        "\n",
        "# Perform Cross-Validation and Hyperparameter Tuning\n",
        "param_grid = {'alpha': [0.1, 1.0, 10.0]}  # Define the hyperparameter grid\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=ridge_model, param_grid=param_grid,\n",
        "                           scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV to find the best degree and alpha\n",
        "grid_search.fit(X_train_poly, y_train)\n",
        "\n",
        "# Get the best degree and alpha from the GridSearchCV results\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = best_model.predict(X_train_poly)\n",
        "test_predictions = best_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Best Alpha:\", best_alpha)\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)\n",
        "\n",
        "# Plot the actual vs. predicted values for the test set\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted values (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HefHlXLpIh0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import  Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "X = data.drop(columns=['Appliances'],axis= 1)\n",
        "y = data['Appliances']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Specify the degree of polynomial (you can change this based on your data)\n",
        "degree = 3\n",
        "\n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=degree)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "X_test_poly = poly_features.transform(X_test)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "lasso_model = Lasso(alpha=1.0)\n",
        "\n",
        "# Train the model using the polynomial features\n",
        "lasso_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = lasso_model.predict(X_train_poly)\n",
        "test_predictions = lasso_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Polynomial Regression (Degree {}):\".format(degree))\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)\n",
        "\n",
        "# Plot the actual vs. predicted values for the test set\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted values (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9UPoqw0BCHk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create a Ridge Regression model\n",
        "lasso_model = Lasso()\n",
        "\n",
        "# Perform Cross-Validation and Hyperparameter Tuning\n",
        "param_grid = {'alpha': [0.1, 1.0, 10.0]}  # Define the hyperparameter grid\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=ridge_model, param_grid=param_grid,\n",
        "                           scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV to find the best degree and alpha\n",
        "grid_search.fit(X_train_poly, y_train)\n",
        "\n",
        "# Get the best degree and alpha from the GridSearchCV results\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = best_model.predict(X_train_poly)\n",
        "test_predictions = best_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Best Alpha:\", best_alpha)\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)\n",
        "\n",
        "# Plot the actual vs. predicted values for the test set\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted values (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AJBf-KWAEOH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5 - elastic net Regression Model"
      ],
      "metadata": {
        "id": "SAu0wU95I9lF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import  ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "X = data.drop(columns=['Appliances'],axis= 1)\n",
        "y = data['Appliances']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Specify the degree of polynomial (you can change this based on your data)\n",
        "degree = 2\n",
        "\n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=degree)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "X_test_poly = poly_features.transform(X_test)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "ElasticNet_model = ElasticNet(alpha=1.0)\n",
        "\n",
        "# Train the model using the polynomial features\n",
        "ElasticNet_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = ElasticNet_model.predict(X_train_poly)\n",
        "test_predictions = ElasticNet_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Polynomial Regression (Degree {}):\".format(degree))\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)\n",
        "\n",
        "# Plot the actual vs. predicted values for the test set\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted values (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A7aJUHSjJEc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create a Ridge Regression model\n",
        "ElasticNet_model = ElasticNet()\n",
        "\n",
        "# Perform Cross-Validation and Hyperparameter Tuning\n",
        "param_grid = {'alpha': [0.1, 1.0, 10.0]}  # Define the hyperparameter grid\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=ridge_model, param_grid=param_grid,\n",
        "                           scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV to find the best degree and alpha\n",
        "grid_search.fit(X_train_poly, y_train)\n",
        "\n",
        "# Get the best degree and alpha from the GridSearchCV results\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = best_model.predict(X_train_poly)\n",
        "test_predictions = best_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Best Alpha:\", best_alpha)\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)\n",
        "\n",
        "# Plot the actual vs. predicted values for the test set\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted values (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "X57hdecTOHvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import  ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "X = data.drop(columns=['Appliances'],axis= 1)\n",
        "y = data['Appliances']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "# Specify the degree of polynomial (you can change this based on your data)\n",
        "degree = 3\n",
        "\n",
        "# Create polynomial features\n",
        "poly_features = PolynomialFeatures(degree=degree)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "X_test_poly = poly_features.transform(X_test)\n",
        "\n",
        "# Create a Linear Regression model\n",
        "ElasticNet_model = ElasticNet(alpha=1.0)\n",
        "\n",
        "# Train the model using the polynomial features\n",
        "ElasticNet_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = ElasticNet_model.predict(X_train_poly)\n",
        "test_predictions = ElasticNet_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Polynomial Regression (Degree {}):\".format(degree))\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)\n",
        "\n",
        "# Plot the actual vs. predicted values for the test set\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted values (Test Set)\")\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "B3stlessNyFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create a Ridge Regression model\n",
        "ElasticNet_model = ElasticNet()\n",
        "\n",
        "# Perform Cross-Validation and Hyperparameter Tuning\n",
        "param_grid = {'alpha': [0.1, 1.0, 10.0]}  # Define the hyperparameter grid\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=ridge_model, param_grid=param_grid,\n",
        "                           scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV to find the best degree and alpha\n",
        "grid_search.fit(X_train_poly, y_train)\n",
        "\n",
        "# Get the best degree and alpha from the GridSearchCV results\n",
        "best_alpha = grid_search.best_params_['alpha']\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = best_model.predict(X_train_poly)\n",
        "test_predictions = best_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Best Alpha:\", best_alpha)\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)\n",
        "\n",
        "# Plot the actual vs. predicted values for the test set\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted values (Test Set)\")\n",
        "plt.show()'''\n"
      ],
      "metadata": {
        "id": "CnBWV3mJOvQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ranfom Forest Regressor"
      ],
      "metadata": {
        "id": "iPocHRDIgkRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Create a Random Forest Regressor model\n",
        "rf_model = RandomForestRegressor(n_estimators=20, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions_rf = rf_model.predict(X_train_poly)\n",
        "test_predictions_rf = rf_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse_rf = mean_squared_error(y_train, train_predictions_rf)\n",
        "test_mse_rf = mean_squared_error(y_test, test_predictions_rf)\n",
        "\n",
        "train_r2_rf = r2_score(y_train, train_predictions_rf)\n",
        "test_r2_rf = r2_score(y_test, test_predictions_rf)\n",
        "\n",
        "print(\"Random Forest Regressor:\")\n",
        "print(\"Train MSE:\", train_mse_rf)\n",
        "print(\"Test MSE:\", test_mse_rf)\n",
        "print(\"Train R-squared:\", train_r2_rf)\n",
        "print(\"Test R-squared:\", test_r2_rf)\n",
        "\n",
        "# Plot the actual vs. predicted values for the test set\n",
        "plt.scatter(y_test, test_predictions_rf)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted values (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CvU_uznThPU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Create a Random Forest Regressor model\n",
        "random_forest_model = RandomForestRegressor()\n",
        "\n",
        "# Perform Cross-Validation and Hyperparameter Tuning\n",
        "param_grid = {'n_estimators': [10, 20],\n",
        "              'max_depth': [None, 10, 20]}  # Define the hyperparameter grid\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=random_forest_model, param_grid=param_grid,\n",
        "                           scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the GridSearchCV to find the best hyperparameters\n",
        "grid_search.fit(X_train_poly, y_train)\n",
        "\n",
        "# Get the best hyperparameters from the GridSearchCV results\n",
        "best_n_estimators = grid_search.best_params_['n_estimators']\n",
        "best_max_depth = grid_search.best_params_['max_depth']\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions = best_model.predict(X_train_poly)\n",
        "test_predictions = best_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse = mean_squared_error(y_train, train_predictions)\n",
        "test_mse = mean_squared_error(y_test, test_predictions)\n",
        "\n",
        "train_r2 = r2_score(y_train, train_predictions)\n",
        "test_r2 = r2_score(y_test, test_predictions)\n",
        "\n",
        "print(\"Best n_estimators:\", best_n_estimators)\n",
        "print(\"Best max_depth:\", best_max_depth)\n",
        "print(\"Train MSE:\", train_mse)\n",
        "print(\"Test MSE:\", test_mse)\n",
        "print(\"Train R-squared:\", train_r2)\n",
        "print(\"Test R-squared:\", test_r2)\n",
        "\n",
        "# Plot the actual vs. predicted values for the test set\n",
        "plt.scatter(y_test, test_predictions)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted values (Test Set)\")\n",
        "plt.show()'''\n"
      ],
      "metadata": {
        "id": "UnzWvHlphBM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###GRADIENT BOOSTING"
      ],
      "metadata": {
        "id": "cv3Xato39JSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Create a Gradient Boosting Regressor model\n",
        "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gb_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions_gb = gb_model.predict(X_train_poly)\n",
        "test_predictions_gb = gb_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse_gb = mean_squared_error(y_train, train_predictions_gb)\n",
        "test_mse_gb = mean_squared_error(y_test, test_predictions_gb)\n",
        "\n",
        "train_r2_gb = r2_score(y_train, train_predictions_gb)\n",
        "test_r2_gb = r2_score(y_test, test_predictions_gb)\n",
        "\n",
        "print(\"Gradient Boosting Regressor:\")\n",
        "print(\"Train MSE:\", train_mse_gb)\n",
        "print(\"Test MSE:\", test_mse_gb)\n",
        "print(\"Train R-squared:\", train_r2_gb)\n",
        "print(\"Test R-squared:\", test_r2_gb)\n",
        "\n",
        "# Plot the actual vs. predicted values for the test set\n",
        "plt.scatter(y_test, test_predictions_gb)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted values (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EhZ-3mgo84eZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBOOST"
      ],
      "metadata": {
        "id": "54RxJBLH9Gq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Create an XGBoost Regressor model\n",
        "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "xgb_model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions on the training and test data\n",
        "train_predictions_xgb = xgb_model.predict(X_train_poly)\n",
        "test_predictions_xgb = xgb_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "train_mse_xgb = mean_squared_error(y_train, train_predictions_xgb)\n",
        "test_mse_xgb = mean_squared_error(y_test, test_predictions_xgb)\n",
        "\n",
        "train_r2_xgb = r2_score(y_train, train_predictions_xgb)\n",
        "test_r2_xgb = r2_score(y_test, test_predictions_xgb)\n",
        "\n",
        "print(\"XGBoost Regressor:\")\n",
        "print(\"Train MSE:\", train_mse_xgb)\n",
        "print(\"Test MSE:\", test_mse_xgb)\n",
        "print(\"Train R-squared:\", train_r2_xgb)\n",
        "print(\"Test R-squared:\", test_r2_xgb)\n",
        "\n",
        "# Plot the actual vs. predicted values for the test set\n",
        "plt.scatter(y_test, test_predictions_xgb)\n",
        "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Actual vs. Predicted values (Test Set)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "C2xGYM3n9DdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Create a Random Forest Regressor object\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, max_features='sqrt')\n",
        "\n",
        "# Fit the Random Forest model to the training data\n",
        "rf_regressor.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = rf_regressor.predict(x_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "rf_regressor.score(x_train, y_train)"
      ],
      "metadata": {
        "id": "KEg4pbOI6EVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_regressor.score(x_train, y_train)"
      ],
      "metadata": {
        "id": "xIl9N_P4Wp4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Create a Random Forest Regressor object\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, max_features='sqrt')\n",
        "\n",
        "# Fit the Random Forest model to the training data\n",
        "rf_regressor.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = rf_regressor.predict(x_test)\n",
        "\n",
        "# Reshape y_train and y_test into 1-dimensional arrays\n",
        "y_train = np.ravel(y_train)\n",
        "y_test = np.ravel(y_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n"
      ],
      "metadata": {
        "id": "4xf4yuab-73N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "\n",
        "# Create an AdaBoostRegressor object\n",
        "ada_regressor = AdaBoostRegressor(n_estimators=100, learning_rate=0.1)\n",
        "\n",
        "# Fit the AdaBoost model to the training data\n",
        "ada_regressor.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = ada_regressor.predict(x_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)"
      ],
      "metadata": {
        "id": "Bpo3SVWi_lg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create a GradientBoostingRegressor object\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,)\n",
        "\n",
        "# Fit the Gradient Boosting model to the training data\n",
        "gb_regressor.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = gb_regressor.predict(x_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)"
      ],
      "metadata": {
        "id": "a0Z3Jj5ZS3LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Create an SVR object\n",
        "svm_regressor = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
        "\n",
        "# Fit the SVR model to the training data\n",
        "svm_regressor.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = svm_regressor.predict(x_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)"
      ],
      "metadata": {
        "id": "7ZWx5Lk_Uz25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaled_data.shape"
      ],
      "metadata": {
        "id": "0bHHrKIO1D_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DECISION TREE"
      ],
      "metadata": {
        "id": "XQpTGjzr3frx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = scaled_data.iloc[:,1:34]\n",
        "y = scaled_data.iloc[:,0:1]"
      ],
      "metadata": {
        "id": "GJfwY_gB0ztE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "RxfwuCMb3kVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "treemodel = DecisionTreeRegressor()\n",
        "\n",
        "treemodel.fit(x_train,y_train)\n",
        "\n",
        "train_score = treemodel.score(x_train,y_train)\n",
        "\n",
        "#prediction stage\n",
        "y_pred = treemodel.predict(x_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n",
        "print(train_score)"
      ],
      "metadata": {
        "id": "Xd2UNtOr4CB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cross validation\n",
        "parameter = { 'criterion' :['squared_error', 'absolute_error'],\n",
        "              'max_depth' : [1,2,3,4,5,6,7,8,9,10]}\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "treemodel =  DecisionTreeRegressor()\n",
        "cv = RandomizedSearchCV(treemodel,param_distributions = parameter, scoring ='neg_mean_squared_error',cv = 5)"
      ],
      "metadata": {
        "id": "AO6795Zw5Kwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "CxjiSg-y7hBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.best_params_"
      ],
      "metadata": {
        "id": "EuQlGlC6BzjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.best_score_"
      ],
      "metadata": {
        "id": "NE6tKtN5dbTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv.score(x_train,y_train)"
      ],
      "metadata": {
        "id": "CkiciefJW5yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = cv.predict(X_test)"
      ],
      "metadata": {
        "id": "UFGSJ6x9DZJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)"
      ],
      "metadata": {
        "id": "IT2TF2UgDfzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = scaled_data.iloc[:,1:34]\n",
        "y = scaled_data.iloc[:,0:1]"
      ],
      "metadata": {
        "id": "PPKqDKFhBqWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###polynomial regression\n"
      ],
      "metadata": {
        "id": "J3FwF2xaFsNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n"
      ],
      "metadata": {
        "id": "UjNgYk3LBkTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poly = PolynomialFeatures(degree=3)\n",
        "X_poly = poly.fit_transform(x_train)"
      ],
      "metadata": {
        "id": "Nh_BgIf5B4Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearRegression()"
      ],
      "metadata": {
        "id": "eNfIB5p0CJ5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5  # Number of folds\n",
        "scores = cross_val_score(model, X_poly, y_train, cv=k, scoring='neg_mean_squared_error')\n",
        "mse_scores = -scores  # Convert negative mean squared error to positive"
      ],
      "metadata": {
        "id": "mtZo3nTxFU-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.score(X_poly, y_train)"
      ],
      "metadata": {
        "id": "tpjkrSqBCtuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data_poly = poly.transform(X_test)\n",
        "predicted_y = model.predict(new_data_poly)"
      ],
      "metadata": {
        "id": "_AO3q1a1CN7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model performance\n",
        "mse = mean_squared_error(y_test, predicted_y)\n",
        "r2 = r2_score(y_test, predicted_y)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n"
      ],
      "metadata": {
        "id": "xUz03lK_Chqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Create a Random Forest Regressor object\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, max_features='sqrt')\n",
        "\n",
        "# Fit the Random Forest model to the training data\n",
        "rf_regressor.fit(x_train,y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = rf_regressor.predict(x_test)\n",
        "\n",
        "# Reshape y_train and y_test into 1-dimensional arrays\n",
        "y_train = np.ravel(y_train)\n",
        "y_test = np.ravel(y_test)\n",
        "\n",
        "# Evaluate the model performance\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "r2 = r2_score(y_test, predictions)\n",
        "\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R-squared:\", r2)\n"
      ],
      "metadata": {
        "id": "Ph1QEqTkGich"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}